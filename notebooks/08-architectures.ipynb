{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "\n",
    "import pretrainedmodels\n",
    "\n",
    "import config\n",
    "import dataset\n",
    "import engine\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18485, 788)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSEUDO-LABELED DATA\n",
    "df = pd.read_csv(\"../input/train_pl2.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONCE\n",
    "# # SHUFFLE\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# # MAKE FOLDS\n",
    "# kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "# for fold, (train_indices, valid_indices) in enumerate(kf.split(df, y=df.digit)):\n",
    "#     np.save(f\"../input/train_idx-fold{fold}\", train_indices)\n",
    "#     np.save(f\"../input/valid_idx-fold{fold}\", valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, list_IDs, augs=None, label=True):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.label = label\n",
    "        self.images = df[config.PIXEL_COLS].values\n",
    "        self.images = self.images.astype(np.uint8)\n",
    "        self.images = self.images.reshape(-1, config.SIZE, config.SIZE, 1)\n",
    "\n",
    "        if label:\n",
    "            self.digits = df.digit.values\n",
    "\n",
    "        if augs is None:\n",
    "            self.augs = A.Compose([\n",
    "                A.Normalize(config.MEAN, config.STD, max_pixel_value=255.0, always_apply=True,),\n",
    "            ])\n",
    "        else:\n",
    "            self.augs = augs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Get image\n",
    "        index = self.list_IDs[item]\n",
    "        image = self.images[index]\n",
    "\n",
    "        # Augment image\n",
    "        image = self.augs(image=image)[\"image\"]\n",
    "\n",
    "        # Convert to PyTorch tensor\n",
    "        image = torch.tensor(image, dtype=torch.float)\n",
    "        image = image.permute(2, 0, 1)\n",
    "        \n",
    "        # Convert to RGB channels\n",
    "        image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Get labels and return\n",
    "        if self.label:\n",
    "            digit = self.digits[index]\n",
    "            digit = torch.tensor(digit, dtype=torch.long)\n",
    "            return image, digit\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"resnet18\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def ResNet34(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def ResNet50(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"resnet50\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def ResNet101(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"resnet101\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def ResNet152(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"resnet152\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * x.sigmoid()\n",
    "\n",
    "\n",
    "def drop_connect(x, drop_ratio):\n",
    "    keep_ratio = 1.0 - drop_ratio\n",
    "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
    "    mask.bernoulli_(keep_ratio)\n",
    "    x.div_(keep_ratio)\n",
    "    x.mul_(mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block with Swish.'''\n",
    "\n",
    "    def __init__(self, in_channels, se_channels):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv2d(in_channels, se_channels,\n",
    "                             kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv2d(se_channels, in_channels,\n",
    "                             kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        out = swish(self.se1(out))\n",
    "        out = self.se2(out).sigmoid()\n",
    "        out = x * out\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''expansion + depthwise + pointwise + squeeze-excitation'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 expand_ratio=1,\n",
    "                 se_ratio=0.,\n",
    "                 drop_rate=0.):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.expand_ratio = expand_ratio\n",
    "\n",
    "        # Expansion\n",
    "        channels = expand_ratio * in_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels,\n",
    "                               channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # Depthwise conv\n",
    "        self.conv2 = nn.Conv2d(channels,\n",
    "                               channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=(1 if kernel_size == 3 else 2),\n",
    "                               groups=channels,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        # SE layers\n",
    "        se_channels = int(in_channels * se_ratio)\n",
    "        self.se = SE(channels, se_channels)\n",
    "\n",
    "        # Output\n",
    "        self.conv3 = nn.Conv2d(channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection if in and out shapes are the same (MV-V2 style)\n",
    "        self.has_skip = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))\n",
    "        out = swish(self.bn2(self.conv2(out)))\n",
    "        out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.has_skip:\n",
    "            if self.training and self.drop_rate > 0:\n",
    "                out = drop_connect(out, self.drop_rate)\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=10):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.conv1 = nn.Conv2d(3,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers = self._make_layers(in_channels=32)\n",
    "        self.linear = nn.Linear(cfg['out_channels'][-1], num_classes)\n",
    "\n",
    "    def _make_layers(self, in_channels):\n",
    "        layers = []\n",
    "        cfg = [self.cfg[k] for k in ['expansion', 'out_channels', 'num_blocks', 'kernel_size',\n",
    "                                     'stride']]\n",
    "        b = 0\n",
    "        blocks = sum(self.cfg['num_blocks'])\n",
    "        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            for stride in strides:\n",
    "                drop_rate = self.cfg['drop_connect_rate'] * b / blocks\n",
    "                layers.append(\n",
    "                    Block(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size,\n",
    "                          stride,\n",
    "                          expansion,\n",
    "                          se_ratio=0.25,\n",
    "                          drop_rate=drop_rate))\n",
    "                in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ZeroPad2d(4)(x)\n",
    "        \n",
    "        out = swish(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        dropout_rate = self.cfg['dropout_rate']\n",
    "        if self.training and dropout_rate > 0:\n",
    "            out = F.dropout(out, p=dropout_rate)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def EfficientNetB0():\n",
    "    cfg = {\n",
    "        'num_blocks': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'expansion': [1, 6, 6, 6, 6, 6, 6],\n",
    "        'out_channels': [16, 24, 40, 80, 112, 192, 320],\n",
    "        'kernel_size': [3, 3, 5, 3, 5, 5, 3],\n",
    "        'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "        'dropout_rate': 0.2,\n",
    "        'drop_connect_rate': 0.2,\n",
    "    }\n",
    "    return EfficientNet(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wide ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "class wide_basic(nn.Module):\n",
    "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
    "        super(wide_basic, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += self.shortcut(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Wide_ResNet(nn.Module):\n",
    "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
    "        super(Wide_ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
    "        n = (depth-4)/6\n",
    "        k = widen_factor\n",
    "\n",
    "        print('| Wide-Resnet %dx%d' %(depth, k))\n",
    "        nStages = [16, 16*k, 32*k, 64*k]\n",
    "\n",
    "        # ADD PADDING\n",
    "        self.pad = nn.ZeroPad2d(4)\n",
    "        \n",
    "        self.conv1 = conv3x3(3,nStages[0])\n",
    "        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
    "        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
    "        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
    "        self.linear = nn.Linear(nStages[3], num_classes)\n",
    "\n",
    "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
    "        strides = [stride] + [1]*(int(num_blocks)-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
    "            self.in_planes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # ADD PADDING\n",
    "        x = self.pad(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def DenseNet121(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"densenet121\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def DenseNet161(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"densenet161\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    model.last_linear = nn.Linear(1024, 10)\n",
    "    return model\n",
    "\n",
    "def DenseNet169(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"densenet169\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model\n",
    "\n",
    "def DenseNet201(pretrained=None):\n",
    "    model = pretrainedmodels.__dict__[\"densenet201\"](pretrained=pretrained)\n",
    "    in_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(in_feats, 10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Model Dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, pretrained=None):\n",
    "    if model_name == \"resnet18\":\n",
    "        return ResNet18(pretrained)\n",
    "    elif model_name == \"resnet34\":\n",
    "        return ResNet34(pretrained)\n",
    "    elif model_name == \"resnet50\":\n",
    "        return ResNet50(pretrained)\n",
    "    elif model_name == \"resnet101\":\n",
    "        return ResNet101(pretrained)\n",
    "    elif model_name == \"resnet152\":\n",
    "        return ResNet152(pretrained)\n",
    "    elif model_name == \"densenet121\":\n",
    "        return DenseNet121(pretrained)\n",
    "    elif model_name == \"densenet161\":\n",
    "        return DenseNet161(pretrained)\n",
    "    elif model_name == \"densenet169\":\n",
    "        return DenseNet169(pretrained)\n",
    "    elif model_name == \"densenet201\":\n",
    "        return DenseNet201(pretrained)\n",
    "    elif model_name == \"wideresnet28x10\":\n",
    "        return Wide_ResNet(28, 10, 0.3, 10)\n",
    "    elif model_name == \"efficientnet\":\n",
    "        return EfficientNetB0(pretrained)\n",
    "    else:\n",
    "        raise RuntimeError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "NUM_FOLDS = 5\n",
    "MODEL_NAME = \"resnet34\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wide ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "oof = np.zeros((len(df), 10))\n",
    "\n",
    "for fold in range(NUM_FOLDS):    \n",
    "    train_indices = np.load(f\"../input/train_idx-fold{fold}\")\n",
    "    valid_indices = np.load(f\"../input/valid_idx-fold{fold}\")\n",
    "    train_ds = EMNISTDataset(df, train_indices)\n",
    "    valid_ds = EMNISTDataset(df, valid_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=512)\n",
    "\n",
    "    model = Wide_ResNet(28, 10, 0.3, 10)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [60, 120, 160], gamma=0.2)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    swa_start = 150\n",
    "    swa_model = AveragedModel(model).to(device)\n",
    "    swa_scheduler = SWALR(optimizer, anneal_strategy=\"cos\", anneal_epochs=5, swa_lr=8e-4)\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        engine.train(train_loader, model, optimizer, device, scaler, clip_grad=False)\n",
    "        preds, targs = engine.evaluate(valid_loader, model, device)\n",
    "        \n",
    "        if epoch < swa_start:\n",
    "            scheduler.step():\n",
    "        else:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_sche\n",
    "\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        accuracy = metrics.accuracy_score(targs, preds)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch={epoch}, Accuracy={accuracy:.5f}\")\n",
    "\n",
    "    update_bn(train_loader, swa_model.cpu())\n",
    "    preds, targs = engine.evaluate(valid_loader, swa_model.to(device), device)\n",
    "    oof[valid_indices] = preds\n",
    "    accuracy = metrics.accuracy_score(targs, np.argmax(preds, axis=1))\n",
    "    print(f\"Fold={fold}, Accuracy={accuracy}\")\n",
    "    \n",
    "    torch.save(swa_model.state_dict(), f\"../models/wide_resnet28x10-sgd-200eps-pl-swa-fold{fold}.pt\")\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "               \n",
    "np.save(f\"oof-wide_resnet28x10-swa\", oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Accuracy=0.11766\n",
      "Epoch=10, Accuracy=0.32594\n",
      "Epoch=20, Accuracy=0.80038\n",
      "Epoch=30, Accuracy=0.81553\n",
      "Epoch=40, Accuracy=0.87557\n",
      "Epoch=50, Accuracy=0.89126\n",
      "Epoch=60, Accuracy=0.88234\n",
      "Epoch=70, Accuracy=0.87585\n",
      "Epoch=80, Accuracy=0.85339\n",
      "Epoch=90, Accuracy=0.88180\n",
      "Epoch=100, Accuracy=0.88694\n",
      "Epoch=110, Accuracy=0.89370\n",
      "Epoch=120, Accuracy=0.90181\n",
      "Epoch=130, Accuracy=0.89099\n",
      "Epoch=140, Accuracy=0.87422\n",
      "Epoch=150, Accuracy=0.93238\n",
      "Epoch=160, Accuracy=0.93887\n",
      "Epoch=170, Accuracy=0.93860\n",
      "Epoch=180, Accuracy=0.94049\n",
      "Epoch=190, Accuracy=0.94157\n",
      "Epoch=200, Accuracy=0.94022\n",
      "Epoch=210, Accuracy=0.94157\n",
      "Epoch=220, Accuracy=0.93941\n",
      "Epoch=230, Accuracy=0.93860\n",
      "Epoch=240, Accuracy=0.93914\n",
      "Epoch=250, Accuracy=0.94076\n",
      "Epoch=260, Accuracy=0.93995\n",
      "Epoch=270, Accuracy=0.94103\n",
      "Epoch=280, Accuracy=0.94049\n",
      "Epoch=290, Accuracy=0.94157\n",
      "Epoch=300, Accuracy=0.93968\n",
      "Epoch=310, Accuracy=0.94184\n",
      "Epoch=320, Accuracy=0.94049\n",
      "Epoch=330, Accuracy=0.93968\n",
      "Epoch=340, Accuracy=0.94049\n",
      "Fold=0, Accuracy=0.9404922910467947\n",
      "Epoch=0, Accuracy=0.16662\n",
      "Epoch=10, Accuracy=0.72789\n",
      "Epoch=20, Accuracy=0.83040\n",
      "Epoch=30, Accuracy=0.85042\n",
      "Epoch=40, Accuracy=0.85421\n",
      "Epoch=50, Accuracy=0.89262\n",
      "Epoch=60, Accuracy=0.85826\n",
      "Epoch=70, Accuracy=0.86367\n",
      "Epoch=80, Accuracy=0.87395\n",
      "Epoch=90, Accuracy=0.86530\n",
      "Epoch=100, Accuracy=0.90668\n",
      "Epoch=110, Accuracy=0.86638\n",
      "Epoch=120, Accuracy=0.88910\n",
      "Epoch=130, Accuracy=0.89721\n",
      "Epoch=140, Accuracy=0.90100\n",
      "Epoch=150, Accuracy=0.93535\n",
      "Epoch=160, Accuracy=0.94212\n",
      "Epoch=170, Accuracy=0.94184\n",
      "Epoch=180, Accuracy=0.94076\n",
      "Epoch=190, Accuracy=0.94212\n",
      "Epoch=200, Accuracy=0.94130\n",
      "Epoch=210, Accuracy=0.94428\n",
      "Epoch=220, Accuracy=0.94482\n",
      "Epoch=230, Accuracy=0.94401\n",
      "Epoch=240, Accuracy=0.94374\n",
      "Epoch=250, Accuracy=0.94428\n",
      "Epoch=260, Accuracy=0.94266\n",
      "Epoch=270, Accuracy=0.94374\n",
      "Epoch=280, Accuracy=0.94509\n",
      "Epoch=290, Accuracy=0.94293\n",
      "Epoch=300, Accuracy=0.94455\n",
      "Epoch=310, Accuracy=0.94401\n",
      "Epoch=320, Accuracy=0.94374\n",
      "Epoch=330, Accuracy=0.94374\n",
      "Epoch=340, Accuracy=0.94347\n",
      "Fold=1, Accuracy=0.9442791452529078\n",
      "Epoch=0, Accuracy=0.14823\n",
      "Epoch=10, Accuracy=0.67893\n",
      "Epoch=20, Accuracy=0.87016\n",
      "Epoch=30, Accuracy=0.85285\n",
      "Epoch=40, Accuracy=0.87693\n",
      "Epoch=50, Accuracy=0.87909\n",
      "Epoch=60, Accuracy=0.84122\n",
      "Epoch=70, Accuracy=0.88666\n",
      "Epoch=80, Accuracy=0.88937\n",
      "Epoch=90, Accuracy=0.87612\n",
      "Epoch=100, Accuracy=0.89532\n",
      "Epoch=110, Accuracy=0.86719\n",
      "Epoch=120, Accuracy=0.90127\n",
      "Epoch=130, Accuracy=0.91128\n",
      "Epoch=140, Accuracy=0.88991\n",
      "Epoch=150, Accuracy=0.93048\n",
      "Epoch=160, Accuracy=0.93860\n",
      "Epoch=170, Accuracy=0.93968\n",
      "Epoch=180, Accuracy=0.93995\n",
      "Epoch=190, Accuracy=0.94103\n",
      "Epoch=200, Accuracy=0.94212\n",
      "Epoch=210, Accuracy=0.94076\n",
      "Epoch=220, Accuracy=0.94239\n",
      "Epoch=230, Accuracy=0.93995\n",
      "Epoch=240, Accuracy=0.94320\n",
      "Epoch=250, Accuracy=0.94184\n",
      "Epoch=260, Accuracy=0.94103\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros((len(df), 10))\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    # DATA\n",
    "    train_indices = np.load(f\"../input/train_idx-fold{fold}.npy\")\n",
    "    valid_indices = np.load(f\"../input/valid_idx-fold{fold}.npy\")\n",
    "    train_ds = EMNISTDataset(df, train_indices)\n",
    "    valid_ds = EMNISTDataset(df, valid_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=512)\n",
    "    \n",
    "    # MODEL and OPTIMIZER\n",
    "    model = get_model(MODEL_NAME).to(device)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [150, 250], gamma=0.1)   \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # SWA\n",
    "    swa_start = 270\n",
    "    swa_model = AveragedModel(model).to(device)\n",
    "    swa_scheduler = SWALR(optimizer, anneal_strategy=\"cos\", anneal_epochs=5, swa_lr=1e-3)\n",
    "    \n",
    "    # TRAINING EPOCHS\n",
    "    for epoch in range(350):\n",
    "        engine.train(train_loader, model, optimizer, device, scaler, clip_grad=False)\n",
    "        preds, targs = engine.evaluate(valid_loader, model, device)\n",
    "        \n",
    "        if epoch > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        accuracy = metrics.accuracy_score(targs, preds)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch={epoch}, Accuracy={accuracy:.5f}\")\n",
    "\n",
    "    # WARMUP BATCH STATISTICS\n",
    "    update_bn(train_loader, swa_model.cpu())\n",
    "    \n",
    "    preds, targs = engine.evaluate(valid_loader, swa_model.to(device), device)\n",
    "    oof[valid_indices] = preds\n",
    "    accuracy = metrics.accuracy_score(targs, np.argmax(preds, axis=1))\n",
    "    print(f\"Fold={fold}, Accuracy={accuracy}\")\n",
    "    \n",
    "    torch.save(swa_model.state_dict(), f\"../models/{MODEL_NAME}-sgd-350eps-pl-swa-{fold}.pt\")\n",
    "\n",
    "    del model, swa_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "np.save(f\"./oof-{MODEL_NAME}-swa\", oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofb = np.argmax(oof, axis=1)\n",
    "accuracy = metrics.accuracy_score(df.digit.values, oofb)\n",
    "print(f\"CV accuracy score={accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(df.digit, oofb)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf, cmap=\"coolwarm\", annot=True, fmt=\"d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.load(f\"./oof-{MODEL_NAME}-swa.npy\")\n",
    "oofb = np.argmax(oof, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP = -0.9\n",
    "s = pd.Series(oofb)\n",
    "vc = s.value_counts().sort_index()\n",
    "mat = np.diag(vc.astype(\"float32\")**(EXP))\n",
    "\n",
    "oofb = np.argmax(oof.dot(mat), axis=1)\n",
    "accuracy = metrics.accuracy_score(df.digit.values, oofb)\n",
    "print(f\"CV accuracy score with Post Process={accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "test_dataset = EMNISTDataset(df_test, np.arange(len(df_test)), label=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_blend = np.zeros((len(df_test), 10))\n",
    "\n",
    "for fold in range(5):\n",
    "    model = get_model(MODEL_NAME)\n",
    "    model = AveragedModel(model).to(device)\n",
    "    model.load_state_dict(torch.load(f\"../models/{MODEL_NAME}-sgd-350eps-pl-swa-{fold}.pt\"))\n",
    "    preds = engine.evaluate(test_loader, model, device, target=False)\n",
    "    preds_blend += preds\n",
    "    \n",
    "np.save(f\"./test-preds-{MODEL_NAME}-sgd-350eps-pl-swa\", preds_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-PROCESSING\n",
    "# COULD SKIP\n",
    "preds_blendb = np.argmax(preds_blend, axis=1)\n",
    "s = pd.Series(preds_blend)\n",
    "vc = s.value_counts().sort_index()\n",
    "mat = np.diag(vc.astype(\"float32\")**(EXP))\n",
    "preds_blend = preds_blend.dot(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsb = np.argmax(preds_blend, axis=1)\n",
    "subm = pd.DataFrame({\"id\": df_test.id, \"digit\": predsb})\n",
    "subm.to_csv(f\"../output/{MODEL_NAME}-sgd-350eps-pl-swa-blend.csv\", index=False)\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_vote = np.zeros((5, len(df_test), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
